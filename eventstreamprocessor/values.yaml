# Default values for central-kms.
# This is a YAML-formatted file.

# Declare global configurations
global: {}

# Declare variables to be passed into your templates.

replicaCount: 1
image:
  repository: mojaloop/event-stream-processor
  tag: v9.5.0-snapshot
  pullPolicy: Always

## Pod scheduling preferences.
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
affinity: {}

## Node labels for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
nodeSelector: {}

## Set toleration for scheduler
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: []

init:
  enabled: true
  kafka:
    name: wait-for-kafka
    repository: solsson/kafka
    tag: latest
    pullPolicy: Always
    command: "until ./bin/kafka-broker-api-versions.sh --bootstrap-server $kafka_host:$kafka_port; do echo waiting for Kafka; sleep 2; done;"
    env: {}
    ## Env example
    # env:
    #   envItem1:
    #     name: hello
    #     value: world
    #

service:
  name: event-stream-processor
  type: ClusterIP
  externalPort: 80
  internalPort: 3082

  annotations: {}

  # This allows one to point the service to an external backend.
  # This is useful for local development where one wishes to hijack
  # the communication from the service to the node layer and point
  # to a specific endpoint (IP, Port, etc).
  external:
    enabled: false
    # 10.0.2.2 is the magic IP for the host on virtualbox's network
    ip: 10.0.2.2
    ports:
      api:
        name: event-stream-processor
        externalPort: 3082

readinessProbe:
  enabled: true
  httpGet:
    path: /health
  initialDelaySeconds: 30
  periodSeconds: 15

livenessProbe:
  enabled: true
  httpGet:
    path: /health
  initialDelaySeconds: 30
  periodSeconds: 15

ingress:
  enabled: false
  # Used to create an Ingress record.
  hosts:
    api: event-stream-processor.local

  externalPath: /

  annotations:
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"

  tls:
    # Secrets must be manually created in the namespace.
    # - secretName: chart-example-tls
    #   hosts:
    #     - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
  #  memory: 128Mi

config:
  PORT: 3082
  ## Kafka Configuration
  # this can be set if the dependency chart for kafka is disabled. If 'kafka_host' is commented out, then the name of the dependency chart will be used.
  # kafka_host: '$release_name-kafka'
  kafka_host: localhost
  kafka_port: 9092
  # fluentd_host: '$release_name-fluentd-elasticsearch-tcp-ingest'
  fluentd_host: efk-fluentd-elasticsearch-tcp-ingest.logging
  fluentd_port: 24224
  fluentd_namespace: logstash
  efk_host: elasticsearch-master.logging:9200
  efk_log: error
  efk_compression: gzip
  efk_index_name: mojaloop
  efk_index_template: '{{index}}-{{date}}'
  # apm_host: '$release_name-apm-server'
  apm_host: efk-apm-server.logging
  apm_port: 8200
  apm_service_name: event-stream-processor
  # master span cache config
  cache:
    ttl: 300000
    segment: trace
    expectSpanTimeout: 2000
    CATBOX_MEMORY:
      partition: trace-cache
      maxByteSize: 524288000
      minCleanupIntervalMsec: 100000

  # master span criteria config
  span:
    START_CRITERIA:
      transfer:
      - prepare:
          service: ml_transfer_prepare
      - fulfil:
          service: ml_transfer_fulfil
      - abort:
          service: ml_transfer_abort
      - timeout-received:
          service: cl_transfer_timeout
    END_CRITERIA:
      transfer:
      - fulfil:
          service: ml_notification_event
      - abort:
          service: ml_notification_event
      - prepare:
          service: ml_notification_event
          isError: true
      - timeout-received:
          service: ml_notification_event
      exceptionList: []
  # log level config
  log_level: debug


# kafka:
#   enabled: true
#   nameOverride: kafka
#   # ------------------------------------------------------------------------------
#   # Kafka:
#   # ------------------------------------------------------------------------------

#   ## The StatefulSet installs 3 pods by default
#   replicas: 1

#   ## The kafka image repository
#   image: "confluentinc/cp-kafka"

#   ## The kafka image tag
#   imageTag: "4.0.1-1"

#   ## Specify a imagePullPolicy
#   ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
#   imagePullPolicy: "IfNotPresent"

#   ## Configure resource requests and limits
#   ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
#   resources: {}
#     # limits:
#     #   cpu: 200m
#     #   memory: 1536Mi
#     # requests:
#     #   cpu: 100m
#     #   memory: 1024Mi
#   kafkaHeapOptions: "-Xmx1G -Xms1G"

#   ## The StatefulSet Update Strategy which Kafka will use when changes are applied.
#   ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
#   updateStrategy:
#     type: "OnDelete"

#   ## Start and stop pods in Parallel or OrderedReady (one-by-one.)  Note - Can not change after first release.
#   ## ref: https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy
#   podManagementPolicy: OrderedReady

#   ## If RBAC is enabled on the cluster, the Kafka init container needs a service account
#   ## with permissisions sufficient to apply pod labels
#   rbac:
#     enabled: true

#   ## The name of the storage class which the cluster should use.
#   # storageClass: default

#   ## The subpath within the Kafka container's PV where logs will be stored.
#   ## This is combined with `persistence.mountPath`, to create, by default: /opt/kafka/data/logs
#   logSubPath: "logs"

#   ## Use an alternate scheduler, e.g. "stork".
#   ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
#   ##
#   # schedulerName:

#   ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
#   ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
#   ## By default we don't set affinity
#   affinity: {}
#   ## Alternatively, this typical example defines:
#   ## antiAffinity (to keep Kafka pods on separate pods)
#   ## and affinity (to encourage Kafka pods to be collocated with Zookeeper pods)
#   # affinity:
#   #   podAntiAffinity:
#   #     requiredDuringSchedulingIgnoredDuringExecution:
#   #     - labelSelector:
#   #         matchExpressions:
#   #         - key: app
#   #           operator: In
#   #           values:
#   #           - kafka
#   #       topologyKey: "kubernetes.io/hostname"
#   #   podAffinity:
#   #     preferredDuringSchedulingIgnoredDuringExecution:
#   #      - weight: 50
#   #        podAffinityTerm:
#   #          labelSelector:
#   #            matchExpressions:
#   #            - key: app
#   #              operator: In
#   #              values:
#   #                - zookeeper
#   #          topologyKey: "kubernetes.io/hostname"

#   ## Node labels for pod assignment
#   ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
#   nodeSelector: {}

#   ## Readiness probe config.
#   ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
#   ##
#   readinessProbe:
#     initialDelaySeconds: 30
#     periodSeconds: 10
#     timeoutSeconds: 5
#     successThreshold: 1
#     failureThreshold: 3

#   ## Period to wait for broker graceful shutdown (sigterm) before pod is killed (sigkill)
#   ## ref: https://kubernetes-v1-4.github.io/docs/user-guide/production-pods/#lifecycle-hooks-and-termination-notice
#   ## ref: https://kafka.apache.org/10/documentation.html#brokerconfigs controlled.shutdown.*
#   terminationGracePeriodSeconds: 60

#   # Tolerations for nodes that have taints on them.
#   # Useful if you want to dedicate nodes to just run kafka
#   # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
#   tolerations: []
#   # tolerations:
#   # - key: "key"
#   #   operator: "Equal"
#   #   value: "value"
#   #   effect: "NoSchedule"

#   ## External access.
#   ##
#   external:
#     enabled: false
#     servicePort: 19092
#     firstListenerPort: 31090
#     domain: cluster.local
#     init:
#       image: "lwolf/kubectl_deployer"
#       imageTag: "0.4"
#       imagePullPolicy: "IfNotPresent"

#   ## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
#   ## here in map format, as defined in the official docs.
#   ## ref: https://kafka.apache.org/documentation/#brokerconfigs
#   ##
#   configurationOverrides:
#     "offsets.topic.replication.factor": 1
#     # "auto.leader.rebalance.enable": true
#     # "auto.create.topics.enable": true
#     # "controlled.shutdown.enable": true
#     # "controlled.shutdown.max.retries": 100

#     ## Options required for external access via NodePort
#     ## ref:
#     ## - http://kafka.apache.org/documentation/#security_configbroker
#     ## - https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
#     ##
#     ## Setting "advertised.listeners" here appends to "PLAINTEXT://${POD_IP}:9092,"
#     # "advertised.listeners": |-
#     #   EXTERNAL://kafka.cluster.local:$((31090 + ${KAFKA_BROKER_ID}))
#     # "listener.security.protocol.map": |-
#     #   PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT

#   ## A collection of additional ports to expose on brokers (formatted as normal containerPort yaml)
#   # Useful when the image exposes metrics (like prometheus, etc.) through a javaagent instead of a sidecar
#   additionalPorts: {}

#   ## Persistence configuration. Specify if and how to persist data to a persistent volume.
#   ##
#   persistence:
#     enabled: false

#     ## The size of the PersistentVolume to allocate to each Kafka Pod in the StatefulSet. For
#     ## production servers this number should likely be much larger.
#     ##
#     size: "1Gi"

#     ## The location within the Kafka container where the PV will mount its storage and Kafka will
#     ## store its logs.
#     ##
#     mountPath: "/opt/kafka/data"

#     ## Kafka data Persistent Volume Storage Class
#     ## If defined, storageClassName: <storageClass>
#     ## If set to "-", storageClassName: "", which disables dynamic provisioning
#     ## If undefined (the default) or set to null, no storageClassName spec is
#     ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
#     ##   GKE, AWS & OpenStack)
#     ##
#     # storageClass:

#   jmx:
#     ## Rules to apply to the Prometheus JMX Exporter.  Note while lots of stats have been cleaned and exposed,
#     ## there are still more stats to clean up and expose, others will never get exposed.  They keep lots of duplicates
#     ## that can be derived easily.  The configMap in this chart cleans up the metrics it exposes to be in a Prometheus
#     ## format, eg topic, broker are labels and not part of metric name. Improvements are gladly accepted and encouraged.
#     configMap:

#       ## Allows disabling the default configmap, note a configMap is needed
#       enabled: true

#       ## Allows setting values to generate confimap
#       ## To allow all metrics through (warning its crazy excessive) comment out below `overrideConfig` and set
#       ## `whitelistObjectNames: []`
#       overrideConfig: {}
#         # jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
#         # lowercaseOutputName: true
#         # lowercaseOutputLabelNames: true
#         # ssl: false
#         # rules:
#         # - pattern: ".*"

#       ## If you would like to supply your own ConfigMap for JMX metrics, supply the name of that
#       ## ConfigMap as an `overrideName` here.
#       overrideName: ""

#     ## Port the jmx metrics are exposed in native jmx format, not in Prometheus format
#     port: 5555

#     ## JMX Whitelist Objects, can be set to control which JMX metrics are exposed.  Only whitelisted
#     ## values will be exposed via JMX Exporter.  They must also be exposed via Rules.  To expose all metrics
#     ## (warning its crazy excessive and they aren't formatted in a prometheus style) (1) `whitelistObjectNames: []`
#     ## (2) commented out above `overrideConfig`.
#     whitelistObjectNames:  # []
#     - kafka.controller:*
#     - kafka.server:*
#     - java.lang:*
#     - kafka.network:*
#     - kafka.log:*

#   ## Prometheus Exporters / Metrics
#   ##
#   prometheus:
#     ## Prometheus JMX Exporter: exposes the majority of Kafkas metrics
#     jmx:
#       enabled: false

#       ## The image to use for the metrics collector
#       image: solsson/kafka-prometheus-jmx-exporter@sha256

#       ## The image tag to use for the metrics collector
#       imageTag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8

#       ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
#       interval: 10s

#       ## Port jmx-exporter exposes Prometheus format metrics to scrape
#       port: 5556

#       resources: {}
#         # limits:
#         #   cpu: 200m
#         #   memory: 1Gi
#         # requests:
#         #   cpu: 100m
#         #   memory: 100Mi

#     ## Prometheus Kafka Exporter: exposes complimentary metrics to JMX Exporter
#     kafka:
#       enabled: false

#       ## The image to use for the metrics collector
#       image: danielqsj/kafka-exporter

#       ## The image tag to use for the metrics collector
#       imageTag: v1.0.1

#       ## Interval at which Prometheus scrapes metrics, note: only used by Prometheus Operator
#       interval: 10s

#       ## Port kafka-exporter exposes for Prometheus to scrape metrics
#       port: 9308

#       ## Resource limits
#       resources: {}
#   #      limits:
#   #        cpu: 200m
#   #        memory: 1Gi
#   #      requests:
#   #        cpu: 100m
#   #        memory: 100Mi

#     operator:
#       ## Are you using Prometheus Operator?
#       enabled: false

#       serviceMonitor:
#         # Namespace Prometheus is installed in
#         namespace: monitoring

#         ## Defaults to whats used if you follow CoreOS [Prometheus Install Instructions](https://github.com/coreos/prometheus-operator/tree/master/helm#tldr)
#         ## [Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/prometheus/templates/prometheus.yaml#L65)
#         ## [Kube Prometheus Selector Label](https://github.com/coreos/prometheus-operator/blob/master/helm/kube-prometheus/values.yaml#L298)
#         selector:
#           prometheus: kube-prometheus

#   # ------------------------------------------------------------------------------
#   # Zookeeper:
#   # ------------------------------------------------------------------------------

#   zookeeper:
#     ## If true, install the Zookeeper chart alongside Kafka
#     ## ref: https://github.com/kubernetes/charts/tree/master/incubator/zookeeper
#     enabled: true

#     ## ref: https://github.com/kubernetes/contrib/tree/master/statefulsets/zookeeper#stateful-set
#     # Desired quantity of ZooKeeper pods. This should always be (1,3,5, or 7)
#     replicaCount: 1

#     ## Configure Zookeeper resource requests and limits
#     ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
#     resources: ~

#     ## The JVM heap size to allocate to Zookeeper
#     heap: "1G"

#     persistence:
#       enabled: false
#       ## The amount of PV storage allocated to each Zookeeper pod in the statefulset
#       # size: "2Gi"

#     ## Specify a Zookeeper imagePullPolicy
#     ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
#     imagePullPolicy: "IfNotPresent"

#     ## If the Zookeeper Chart is disabled a URL and port are required to connect
#     url: ""
#     port: 2181

#     ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
#     ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
#     ## By default we don't set affinity:
#     affinity: {}  # Criteria by which pod label-values influence scheduling for zookeeper pods.
#     # podAntiAffinity:
#     #   requiredDuringSchedulingIgnoredDuringExecution:
#     #     - topologyKey: "kubernetes.io/hostname"
#     #       labelSelector:
#     #         matchLabels:
#     #           release: zookeeper
