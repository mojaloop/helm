# Default values for mojaloop-efk.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global: {}

fluentd-elasticsearch:
  enabled: true
  image:
    repository: quay.io/fluentd_elasticsearch/fluentd
  ## Specify an imagePullPolicy (Required)
  ## It's recommended to change this to 'Always' if the image tag is 'latest'
  ## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
    tag: v3.2.0
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistrKeySecretName

  ## If using AWS Elasticsearch, all requests to ES need to be signed regardless of whether
  ## one is using Cognito or not. By setting this to true, this chart will install a sidecar
  ## proxy that takes care of signing all requests being sent to the AWS ES Domain.
  awsSigningSidecar:
    enabled: false
    resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 500Mi
    # requests:
    #   cpu: 100m
    #   memory: 200Mi
    network:
      port: 8080
      address: localhost
      remoteReadTimeoutSeconds: 15
    image:
      repository: abutaha/aws-es-proxy
      tag: v1.0

  # Specify to use specific priorityClass for pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  # If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority
  # Pods to make scheduling of the pending Pod possible.
  priorityClassName: ""

  # Specify where fluentd can find logs
  hostLogDir:
    varLog: /var/log
    dockerContainers: /var/lib/docker/containers
    libSystemdDir: /usr/lib64

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 500Mi
    # requests:
    #   cpu: 100m
    #   memory: 200Mi

  elasticsearch:
    auth:
      enabled: false
      user: "yourUser"
      password: "yourPass"
    includeTagKey: true
    setOutputHostEnvVar: true
    # If setOutputHostEnvVar is false this value is ignored
    hosts: ["elasticsearch-master:9200"]
    indexName: "fluentd"
    logstash:
      enabled: true
      prefix: "logstash"
      prefixSeparator: "-"
      dateformat: "%Y.%m.%d"
    ilm:
      enabled: false
      policy_id: logstash-policy
      policy: {}
        # example for ilm policy config
        # phases:
        #   hot:
        #     min_age: 0ms
        #     actions:
        #       rollover:
        #         max_age: 30d
        #         max_size: 20gb
        #       set_priority:
        #           priority: 100
        #   delete:
        #     min_age: 60d
        #     actions:
        #       delete:
      policies: {}
        # example for ilm policies config
        # ilm_policy_id1: {}
        # ilm_policy_id2: {}
      policy_overwrite: false
    path: ""
    scheme: "http"
    sslVerify: true
    sslVersion: "TLSv1_2"
    outputType: "elasticsearch"
    typeName: "_doc"
    logLevel: "info"
    reconnectOnError: true
    reloadOnFailure: false
    reloadConnections: false
    requestTimeout: "5s"
    suppressTypeName: false
    buffer:
      enabled: true
      type: "file"
      path: "/var/log/fluentd-buffers/kubernetes.system.buffer"
      flushMode: "interval"
      retryType: "exponential_backoff"
      flushThreadCount: 2
      flushInterval: "5s"
      retryForever: true
      retryMaxInterval: 30
      chunkLimitSize: "2M"
      queueLimitLength: 8
      overflowAction: "block"

  # If you want to change args of fluentd process
  # by example you can add -vv to launch with trace log
  fluentdArgs: "--no-supervisor -q"

  # If you want to add custom environment variables, use the env dict
  # You can then reference these in your config file e.g.:
  #     user "#{ENV['OUTPUT_USER']}"
  env: {}
    # OUTPUT_USER: my_user
    # LIVENESS_THRESHOLD_SECONDS: 300
    # STUCK_THRESHOLD_SECONDS: 900

  # If you want to add custom environment variables from secrets, use the secret list
  secret: []
  # - name: ELASTICSEARCH_PASSWORD
  #   secret_name: elasticsearch
  #   secret_key: password

  rbac:
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
    annotations: {}

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    enabled: false
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  livenessProbe:
    enabled: true
    initialDelaySeconds: 600
    periodSeconds: 60
    kind:
      exec:
        command:
        # Liveness probe is aimed to help in situations where fluentd
        # silently hangs for no apparent reasons until manual restart.
        # The idea of this probe is that if fluentd is not queueing or
        # flushing chunks for 5 minutes, something is not right. If
        # you want to change the fluentd configuration, reducing amount of
        # logs fluentd collects, consider changing the threshold or turning
        # liveness probe off completely.
        - '/bin/sh'
        - '-c'
        - >
          LIVENESS_THRESHOLD_SECONDS=${LIVENESS_THRESHOLD_SECONDS:-300};
          STUCK_THRESHOLD_SECONDS=${STUCK_THRESHOLD_SECONDS:-900};
          if [ ! -e /var/log/fluentd-buffers ];
          then
            exit 1;
          fi;
          touch -d "${STUCK_THRESHOLD_SECONDS} seconds ago" /tmp/marker-stuck;
          if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-stuck -print -quit)" ];
          then
            rm -rf /var/log/fluentd-buffers;
            exit 1;
          fi;
          touch -d "${LIVENESS_THRESHOLD_SECONDS} seconds ago" /tmp/marker-liveness;
          if [ -z "$(find /var/log/fluentd-buffers -type d -newer /tmp/marker-liveness -print -quit)" ];
          then
            exit 1;
          fi;

  annotations: {}

  podAnnotations: {}
    # prometheus.io/scrape: "true"
    # prometheus.io/port: "24231"

  ## DaemonSet update strategy
  ## Ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
  updateStrategy:
    type: RollingUpdate

  tolerations: []
    # - key: node-role.kubernetes.io/master
    #   operator: Exists
    #   effect: NoSchedule

  affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
    #       - key: node-role.kubernetes.io/master
    #         operator: DoesNotExist

  nodeSelector: {}

  service: {}
    # ports:
    #   - name: "monitor-agent"
    #     type: ClusterIP
    #     port: 24231

  serviceMonitor:
    ## If true, a ServiceMonitor CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    interval: 10s
    path: /metrics
    port: 24231
    labels: {}
    metricRelabelings: []
    relabelings: []
    jobLabel: "app.kubernetes.io/instance"
    type: ClusterIP

  serviceMetric:
    ## If true, the metrics service will be created
    ## Alternative to implicit creation through serviceMonitor.enabled
    ##
    enabled: false

  prometheusRule:
    ## If true, a PrometheusRule CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    prometheusNamespace: monitoring
    labels: {}
    #  role: alert-rules

  configMaps:
    useDefaults:
      systemConf: true
      containersInputConf: true
      systemInputConf: true
      forwardInputConf: true
      monitoringConf: true
      outputConf: true

  # can be used to add new config or overwrite the default configmaps completely after the configmaps default has been disabled above
  extraConfigMaps: {}
    # system.conf: |-
    #   <system>
    #     root_dir /tmp/fluentd-buffers/
    #   </system>

  extraVolumes: []
  #   - name: es-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: es-certs

  extraVolumeMounts: []
  #   - name: es-certs
  #     mountPath: /certs
  #     readOnly: true

elasticsearch:
  # Default values for elasticsearch.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  enabled: true

  clusterName: "elasticsearch"
  nodeGroup: "master"

  # The service that non master groups will try to connect to when joining the cluster
  # This should be set to clusterName + "-" + nodeGroup for your master group
  masterService: ""

  # Elasticsearch roles that will be applied to this nodeGroup
  # These will be set as environment variables. E.g. node.master=true
  roles:
    master: "true"
    ingest: "true"
    data: "true"

  replicas: 3
  minimumMasterNodes: 2

  esMajorVersion: ""

  # Allows you to add any config files in /usr/share/elasticsearch/config/
  # such as elasticsearch.yml and log4j2.properties
  esConfig: {}
  #  elasticsearch.yml: |
  #    key:
  #      nestedkey: value
  #  log4j2.properties: |
  #    key = value

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # Allows you to load environment variables from kubernetes secret or config map
  envFrom: []
  # - secretRef:
  #     name: env-secret
  # - configMapRef:
  #     name: config-map

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/elasticsearch/config/certs
  #    defaultMode: 0755

  image: "docker.elastic.co/elasticsearch/elasticsearch"
  imageTag: "7.11.1"
  imagePullPolicy: "IfNotPresent"

  podAnnotations: {}
    # iam.amazonaws.com/role: es-cluster

  # additionals labels
  labels: {}

  esJavaOpts: "-Xmx1g -Xms1g"

  resources:
    requests:
      cpu: "100m"
      memory: "2Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  initResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
    # requests:
    #   cpu: "25m"
    #   memory: "128Mi"

  sidecarResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
    # requests:
    #   cpu: "25m"
    #   memory: "128Mi"

  networkHost: "0.0.0.0"

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 30Gi

  rbac:
    create: false
    serviceAccountAnnotations: {}
    serviceAccountName: ""

  # podSecurityContext: {}
  #   fsGroup: 1000
  #   runAsUser: 1000
  #   runAsGroup: 0

  persistence:
    enabled: false
    labels:
      # Add default labels for the volumeClaimTemplate fo the StatefulSet
      enabled: false
    annotations: {}

  extraVolumes: []
    # - name: extras
    #   emptyDir: {}

  extraVolumeMounts: []
    # - name: extras
    #   mountPath: /usr/share/extras
    #   readOnly: true

  extraContainers: []
    # - name: do-something
    #   image: busybox
    #   command: ['do', 'something']

  extraInitContainers: []
    # - name: do-something
    #   image: busybox
    #   command: ['do', 'something']

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # By default this will make sure two pods don't end up on the same node
  # Changing this to a region would allow you to spread pods across regions
  antiAffinityTopologyKey: "kubernetes.io/hostname"

  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "hard"

  # This is the node affinity settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
  nodeAffinity: {}

  # The default is to deploy all pods serially. By setting this to parallel all pods are started at
  # the same time when bootstrapping the cluster
  podManagementPolicy: "Parallel"

  # The environment variables injected by service links are not used, but can lead to slow Elasticsearch boot times when
  # there are many services in the current namespace.
  # If you experience slow pod startups you probably want to set this to `false`.
  enableServiceLinks: true

  protocol: http
  httpPort: 9200
  transportPort: 9300

  service:
    labels: {}
    labelsHeadless: {}
    type: ClusterIP
    nodePort: ""
    annotations: {}
    httpPortName: http
    transportPortName: transport
    loadBalancerIP: ""
    loadBalancerSourceRanges: []

  updateStrategy: RollingUpdate

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 1

  # podSecurityContext: {}
    # fsGroup: 1000
    # runAsUser: 1000

  securityContext:
    capabilities:
      drop:
      - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  # How long to wait for elasticsearch to stop gracefully
  terminationGracePeriod: 120

  sysctlVmMaxMapCount: 262144

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  # https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params wait_for_status
  clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

  ## Use an alternate scheduler.
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []

  # Enabling this will publically expose your Elasticsearch instance.
  # Only enable this if you have security enabled on your cluster
  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - elasticsearch.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  nameOverride: ""
  fullnameOverride: ""

  # https://github.com/elastic/helm-charts/issues/63
  masterTerminationFix: false

  lifecycle: {}
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # postStart:
    #   exec:
    #     command:
    #       - bash
    #       - -c
    #       - |
    #         #!/bin/bash
    #         # Add a template to adjust number of shards/replicas
    #         TEMPLATE_NAME=my_template
    #         INDEX_PATTERN="logstash-*"
    #         SHARD_COUNT=8
    #         REPLICA_COUNT=1
    #         ES_URL=http://localhost:9200
    #         while [[ "$(curl -s -o /dev/null -w '%{http_code}\n' $ES_URL)" != "200" ]]; do sleep 1; done
    #         curl -XPUT "$ES_URL/_template/$TEMPLATE_NAME" -H 'Content-Type: application/json' -d'{"index_patterns":['\""$INDEX_PATTERN"\"'],"settings":{"number_of_shards":'$SHARD_COUNT',"number_of_replicas":'$REPLICA_COUNT'}}'

  sysctlInitContainer:
    enabled: true

  keystore: []

  # Deprecated
  # please use the above podSecurityContext.fsGroup instead
  fsGroup: ""

kibana:
  enabled: true

  elasticsearchHosts: "http://elasticsearch-master:9200"

  replicas: 1

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs:
    - name: "NODE_OPTIONS"
      value: "--max-old-space-size=1800"
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # Allows you to load environment variables from kubernetes secret or config map
  envFrom: []
  # - secretRef:
  #     name: env-secret
  # - configMapRef:
  #     name: config-map

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: kibana-keystore
  #    secretName: kibana-keystore
  #    path: /usr/share/kibana/data/kibana.keystore
  #    subPath: kibana.keystore # optional

  image: "docker.elastic.co/kibana/kibana"
  imageTag: "7.11.1"
  imagePullPolicy: "IfNotPresent"

  # additionals labels
  labels: {}

  podAnnotations: {}
    # iam.amazonaws.com/role: es-cluster

  resources:
    requests:
      cpu: "1000m"
      memory: "2Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  protocol: http

  serverHost: "0.0.0.0"

  healthCheckPath: "/app/kibana"

  # Allows you to add any config files in /usr/share/kibana/config/
  # such as kibana.yml
  kibanaConfig: {}
  #   kibana.yml: |
  #     key:
  #       nestedkey: value

  # If Pod Security Policy in use it may be required to specify security context as well as service account

  # podSecurityContext: {}
    # fsGroup: 1000

  securityContext:
    capabilities:
      drop:
      - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  serviceAccount: ""

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  httpPort: 5601

  extraContainers: ""
  # - name: dummy-init
  #   image: busybox
  #   command: ['echo', 'hey']

  extraInitContainers: ""
  # - name: dummy-init
  #   image: busybox
  #   command: ['echo', 'hey']

  updateStrategy:
    type: "Recreate"

  service:
    type: ClusterIP
    port: 5601
    nodePort: ""
    labels: {}
    annotations: {}
      # cloud.google.com/load-balancer-type: "Internal"
      # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
      # service.beta.kubernetes.io/azure-load-balancer-internal: "true"
      # service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
      # service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"
    loadBalancerSourceRanges: []
      # 0.0.0.0/0

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - kibana.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []
  affinity: {}

  nameOverride: ""
  fullnameOverride: ""

  lifecycle: {}
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

  # Deprecated - use only with versions < 6.6
  # elasticsearchURL: "" # "http://elasticsearch-master:9200"

apm-server:
  enabled: true

  apmConfig:
    apm-server.yml: |
      apm-server:
        host: "0.0.0.0:8200"

      queue: {}

      output.elasticsearch:
        hosts: ["http://elasticsearch-master:9200"]
        ## If you have security enabled- you'll need to add the credentials
        ## as environment variables
        # username: "${ELASTICSEARCH_USERNAME}"
        # password: "${ELASTICSEARCH_PASSWORD}"
        ## If SSL is enabled
        # protocol: https
        # ssl.certificate_authorities:
        #  - /usr/share/apm-server/config/certs/elastic-ca.pem

  replicas: 1

  extraContainers: ""
  # - name: dummy-init
  #   image: busybox
  #   command: ['echo', 'hey']

  extraInitContainers: ""
  # - name: dummy-init
  #   image: busybox
  #   command: ['echo', 'hey']

  # Extra environment variables to append to the DaemonSet pod spec.
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
    #  - name: 'ELASTICSEARCH_USERNAME'
    #    valueFrom:
    #      secretKeyRef:
    #        name: elastic-credentials
    #        key: username
    #  - name: 'ELASTICSEARCH_PASSWORD'
    #    valueFrom:
    #      secretKeyRef:
    #        name: elastic-credentials
    #        key: password

  # Allows you to load environment variables from kubernetes secret or config map
  envFrom: []
  # - secretRef:
  #     name: env-secret
  # - configMapRef:
  #     name: config-map

  extraVolumeMounts: []
    # - name: extras
    #   mountPath: /usr/share/extras
    #   readOnly: true

  extraVolumes: []
    # - name: extras
    #   emptyDir: {}

  image: "docker.elastic.co/apm/apm-server"
  imageTag: "7.11.1"
  imagePullPolicy: "IfNotPresent"
  imagePullSecrets: []

  # Whether this chart should self-manage its service account, role, and associated role binding.
  managedServiceAccount: true

  podAnnotations: {}
    # iam.amazonaws.com/role: es-cluster

  # additionals labels
  labels: {}

  # podSecurityContext: {}
    # runAsUser: 0
    # privileged: false

  livenessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  resources:
      requests:
        cpu: "100m"
        memory: "100Mi"
      limits:
        cpu: "1000m"
        memory: "200Mi"

  # Custom service account override that the pod will use
  serviceAccount: ""

  # A list of secrets and their paths to mount inside the pod
  secretMounts: []
  #  - name: elastic-certificate-pem
  #    secretName: elastic-certificates
  #    path: /usr/share/apm-server/config/certs

  terminationGracePeriod: 30

  tolerations: []

  nodeSelector: {}

  affinity: {}

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  updateStrategy:
    type: "RollingUpdate"

  # Override various naming aspects of this chart
  # Only edit these if you know what you're doing
  nameOverride: ""
  fullnameOverride: ""

  autoscaling:
    enabled: false

  ingress:
    enabled: false
    annotations: {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - chart-example.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  service:
    type: ClusterIP
    port: 8200
    nodePort: ""
    annotations: {}
      # cloud.google.com/load-balancer-type: "Internal"
      # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
      # service.beta.kubernetes.io/azure-load-balancer-internal: "true"
      # service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
      # service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"

  lifecycle: {}
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    # postStart:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

