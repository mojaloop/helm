# Default values for mojaloop-efk.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global: {}

# Declare variables to be passed into your templates.
fluentd-elasticsearch:
  image:
    repository: quay.io/fluentd_elasticsearch/fluentd
    ## Specify an imagePullPolicy (Required)
    ## It's recommended to change this to 'Always' if the image tag is 'latest'
    ## ref: http://kubernetes.io/docs/user-guide/images/#updating-images
    tag: v2.6.0
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    # pullSecrets:
    #   - myRegistrKeySecretName

  ## If using AWS Elasticsearch, all requests to ES need to be signed regardless of whether
  ## one is using Cognito or not. By setting this to true, this chart will install a sidecar
  ## proxy that takes care of signing all requests being sent to the AWS ES Domain.
  awsSigningSidecar:
    enabled: false
    image:
      repository: abutaha/aws-es-proxy
      tag: 0.9

  # Specify to use specific priorityClass for pods
  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  # If a Pod cannot be scheduled, the scheduler tries to preempt (evict) lower priority
  # Pods to make scheduling of the pending Pod possible.
  priorityClassName: ""

  # Specify where fluentd can find logs
  hostLogDir:
    varLog: /var/log
    dockerContainers: /var/lib/docker/containers
    libSystemdDir: /usr/lib64

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 500Mi
    # requests:
  #   cpu: 100m
  #   memory: 200Mi

  elasticsearch:
    auth:
      enabled: false
      user: "yourUser"
      password: "yourPass"
    bufferChunkLimit: "2M"
    bufferQueueLimit: 8
    host: "elasticsearch-master"
    logstashPrefix: "logstash"
    port: 9200
    scheme: "http"
    sslVerify: true
    sslVersion: "TLSv1_2"

  # If you want to change args of fluentd process
  # by example you can add -vv to launch with trace log
  fluentdArgs: "--no-supervisor -q"

  # If you want to add custom environment variables, use the env dict
  # You can then reference these in your config file e.g.:
  #     user "#{ENV['OUTPUT_USER']}"
  env:
  # OUTPUT_USER: my_user
  # LIVENESS_THRESHOLD_SECONDS: 300
  # STUCK_THRESHOLD_SECONDS: 900

  # If you want to add custom environment variables from secrets, use the secret list
  secret:
  # - name: ELASTICSEARCH_PASSWORD
  #   secret_name: elasticsearch
  #   secret_key: password

  rbac:
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  ## Specify if a Pod Security Policy for node-exporter must be created
  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/
  ##
  podSecurityPolicy:
    enabled: false
    annotations: {}
      ## Specify pod annotations
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
      ##
      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
    # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
    # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

  livenessProbe:
    enabled: true

  annotations: {}

  podAnnotations: {}
  # prometheus.io/scrape: "true"
  # prometheus.io/port: "24231"

  ## DaemonSet update strategy
  ## Ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
  updateStrategy:
    type: RollingUpdate

  tolerations: {}
    # - key: node-role.kubernetes.io/master
  #   operator: Exists
  #   effect: NoSchedule

  affinity: {}
    # nodeAffinity:
    #   requiredDuringSchedulingIgnoredDuringExecution:
    #     nodeSelectorTerms:
    #     - matchExpressions:
  #       - key: node-role.kubernetes.io/master
  #         operator: DoesNotExist

  nodeSelector: {}

  service:
    ports:
      - name: "monitor-agent"
        type: ClusterIP
        port: 24231
      - name: "tcp-ingest"
        type: ClusterIP
        port: 24224
  #      - name: "http-ingest"
  #        type: ClusterIP
  #        port: 9880

  serviceMonitor:
    ## If true, a ServiceMonitor CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    interval: 10s
    path: /metrics
    port: 24231
    labels: {}

  prometheusRule:
    ## If true, a PrometheusRule CRD is created for a prometheus operator
    ## https://github.com/coreos/prometheus-operator
    ##
    enabled: false
    prometheusNamespace: monitoring
    labels: {}
    #  role: alert-rules

  configMaps:
    useDefaults:
      systemConf: true
      containersInputConf: true
      systemInputConf: true
      forwardInputConf: false
      monitoringConf: true
      outputConf: true

  # can be used to add new config or overwrite the default configmaps completely after the configmaps default has been disabled above
  extraConfigMaps:
    #   system.conf: |-
    #     <system>
    #       root_dir /tmp/fluentd-buffers/
    #     </system>
    forward.input.conf: |-
      # Forwards the messages sent over TCP and sends it to elasticsearch - https://docs.fluentd.org/input/forward
      <source>
        @id forward
        @type forward
        bind 0.0.0.0
        port 24224
      </source>
  #      # Takes the messages sent over HTTP and sends it to elasticsearch - https://docs.fluentd.org/input/http
  #      <source>
  #        @type http
  #        bind 0.0.0.0
  #        port 9880
  #        body_size_limit 32m
  #        keepalive_timeout 10s
  #      </source>

  # extraVolumes:
  #   - name: es-certs
  #     secret:
  #       defaultMode: 420
  #       secretName: es-certs
  # extraVolumeMounts:
  #   - name: es-certs
  #     mountPath: /certs
  #     readOnly: true

elasticsearch:
  # Default values for elasticsearch.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  clusterName: "elasticsearch"
  nodeGroup: "master"

  # The service that non master groups will try to connect to when joining the cluster
  # This should be set to clusterName + "-" + nodeGroup for your master group
  masterService: ""

  # Elasticsearch roles that will be applied to this nodeGroup
  # These will be set as environment variables. E.g. node.master=true
  roles:
    master: "true"
    ingest: "true"
    data: "true"

  replicas: 3
  minimumMasterNodes: 2

  esMajorVersion: ""

  # Allows you to add any config files in /usr/share/elasticsearch/config/
  # such as elasticsearch.yml and log4j2.properties
  esConfig: {}
  #  elasticsearch.yml: |
  #    key:
  #      nestedkey: value
  #  log4j2.properties: |
  #    key = value

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/elasticsearch/config/certs

  image: "docker.elastic.co/elasticsearch/elasticsearch"
  imageTag: "7.2.0"
  imagePullPolicy: "IfNotPresent"

  podAnnotations: {}
  # iam.amazonaws.com/role: es-cluster

  esJavaOpts: "-Xmx1g -Xms1g"

  resources:
    requests:
      cpu: "100m"
      memory: "2Gi"
    limits:
      cpu: "1000m"
      memory: "2Gi"

  initResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

  sidecarResources: {}
    # limits:
    #   cpu: "25m"
    #   # memory: "128Mi"
  # requests:
  #   cpu: "25m"
  #   memory: "128Mi"

  networkHost: "0.0.0.0"

  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: "awsgp2"
    resources:
      requests:
        storage: 100Gi

  persistence:
    enabled: false
    annotations: {}

  extraVolumes: []
  # - name: extras
  #   emptyDir: {}

  extraVolumeMounts: []
  # - name: extras
  #   mountPath: /usr/share/extras
  #   readOnly: true

  extraInitContainers: []
  # - name: do-something
  #   image: busybox
  #   command: ['do', 'something']

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # By default this will make sure two pods don't end up on the same node
  # Changing this to a region would allow you to spread pods across regions
  antiAffinityTopologyKey: "kubernetes.io/hostname"

  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "hard"

  # This is the node affinity settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity-beta-feature
  nodeAffinity: {}

  # The default is to deploy all pods serially. By setting this to parallel all pods are started at
  # the same time when bootstrapping the cluster
  podManagementPolicy: "Parallel"

  protocol: http
  httpPort: 9200
  transportPort: 9300

  service:
    type: ClusterIP
    nodePort:
    annotations: {}

  updateStrategy: RollingUpdate

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 1

  podSecurityContext:
    fsGroup: 1000

  # The following value is deprecated,
  # please use the above podSecurityContext.fsGroup instead
  fsGroup: ""

  securityContext:
    capabilities:
      drop:
        - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  # How long to wait for elasticsearch to stop gracefully
  terminationGracePeriod: 120

  sysctlVmMaxMapCount: 262144

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  # https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html#request-params wait_for_status
  clusterHealthCheckParams: "wait_for_status=green&timeout=1s"

  ## Use an alternate scheduler.
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []

  # Enabling this will publically expose your Elasticsearch instance.
  # Only enable this if you have security enabled on your cluster
  ingress:
    enabled: false
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - elasticsearch.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  nameOverride: ""
  fullnameOverride: ""

  # https://github.com/elastic/helm-charts/issues/63
  masterTerminationFix: false

  lifecycle: {}
    # preStop:
    #   exec:
    #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
  # postStart:
  #   exec:
  #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]

  sysctlInitContainer:
    enabled: true

kibana:
  elasticsearchURL: "" # "http://elasticsearch-master:9200"
  elasticsearchHosts: "http://elasticsearch-master:9200"

  replicas: 1

  # Extra environment variables to append to this nodeGroup
  # This will be appended to the current 'env:' key. You can use any of the kubernetes env
  # syntax here
  extraEnvs: []
  #  - name: MY_ENVIRONMENT_VAR
  #    value: the_value_goes_here

  # A list of secrets and their paths to mount inside the pod
  # This is useful for mounting certificates for security and for mounting
  # the X-Pack license
  secretMounts: []
  #  - name: elastic-certificates
  #    secretName: elastic-certificates
  #    path: /usr/share/elasticsearch/config/certs

  image: "docker.elastic.co/kibana/kibana"
  imageTag: "7.2.0"
  imagePullPolicy: "IfNotPresent"

  podAnnotations: {}
  # iam.amazonaws.com/role: es-cluster

  resources:
    requests:
      cpu: "100m"
      memory: "500m"
    limits:
      cpu: "1000m"
      memory: "1Gi"

  protocol: http

  serverHost: "0.0.0.0"

  healthCheckPath: "/app/kibana"

  # Allows you to add any config files in /usr/share/kibana/config/
  # such as kibana.yml
  kibanaConfig: {}
  #   kibana.yml: |
  #     key:
  #       nestedkey: value

  # If Pod Security Policy in use it may be required to specify security context as well as service account

  podSecurityContext:
    fsGroup: 1000

  securityContext:
    capabilities:
      drop:
        - ALL
    # readOnlyRootFilesystem: true
    runAsNonRoot: true
    runAsUser: 1000

  serviceAccount: ""

  # This is the PriorityClass settings as defined in
  # https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/#priorityclass
  priorityClassName: ""

  # By default this will make sure two pods don't end up on the same node
  # Changing this to a region would allow you to spread pods across regions
  antiAffinityTopologyKey: "kubernetes.io/hostname"

  # Hard means that by default pods will only be scheduled if there are enough nodes for them
  # and that they will never end up on the same node. Setting this to soft will do this "best effort"
  antiAffinity: "hard"

  httpPort: 5601

  # This is the max unavailable setting for the pod disruption budget
  # The default value of 1 will make sure that kubernetes won't allow more than 1
  # of your pods to be unavailable during maintenance
  maxUnavailable: 1

  updateStrategy:
    type: "Recreate"

  service:
    type: ClusterIP
    port: 5601
    nodePort:
    annotations: {}
      # cloud.google.com/load-balancer-type: "Internal"
      # service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0
    # service.beta.kubernetes.io/azure-load-balancer-internal: "true"
    # service.beta.kubernetes.io/openstack-internal-load-balancer: "true"
    # service.beta.kubernetes.io/cce-load-balancer-internal-vpc: "true"

  ingress:
    enabled: true
    annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
    path: /
    hosts:
      - kibana.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  readinessProbe:
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    successThreshold: 3
    timeoutSeconds: 5

  imagePullSecrets: []
  nodeSelector: {}
  tolerations: []
  affinity: {}

  nameOverride: ""
  fullnameOverride: ""

apm-server:
  image:
    repository: docker.elastic.co/apm/apm-server
    tag: 7.0.0
    pullPolicy: IfNotPresent

  # DaemonSet or Deployment
  kind: DaemonSet

  # Number of replicas when kind is Deployment
  replicaCount: 1

  # Create HorizontalPodAutoscaler object when kind is Deployment
  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 10
    metrics:
      - type: Resource
        resource:
          name: cpu
          targetAverageUtilization: 60
      - type: Resource
        resource:
          name: memory
          targetAverageUtilization: 60

  # The update strategy to apply to the Deployment or DaemonSet
  updateStrategy: {}
  # rollingUpdate:
  #   maxUnavailable: 1
  # type: RollingUpdate

  service:
    enabled: true
    type: ClusterIP
    port: 8200
    # portName: apm-server-svc
    # clusterIP: None
    ## External IP addresses of service
    ## Default: nil
    # externalIPs:
    # - 192.168.0.1
    #
    ## LoadBalancer IP if service.type is LoadBalancer
    ## Default: nil
    # loadBalancerIP: 10.2.2.2
    ## Limit load balancer source ips to list of CIDRs (where available)
    # loadBalancerSourceRanges: []

    annotations: {}
    # Annotation example: setup ssl with aws cert when service.type is LoadBalancer
    # service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-east-1:EXAMPLE_CERT
    labels: {}
    ## Label example: show service URL in `kubectl cluster-info`
    # kubernetes.io/cluster-service: "true"

  ingress:
    enabled: false
    annotations: {}
    # Annotation example: set nginx ingress type
    # kubernetes.io/ingress.class: nginx-public
    labels: {}
    hosts:
      - apm-server.local
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  config:
    apm-server:
      ### Defines the host and port the server is listening on
      host: "0.0.0.0:8200"

      ## Maximum permitted size in bytes of an unzipped request accepted by the server to be processed.
      # max_unzipped_size: 52428800
      ## Maximum permitted size in bytes of a request's header accepted by the server to be processed.
      # max_header_size: 1048576

      ## Maximum permitted duration in seconds for reading an entire request.
      # read_timeout: 2s
      ## Maximum permitted duration in seconds for writing a response.
      # write_timeout: 2s

      ## Maximum duration in seconds before releasing resources when shutting down the server.
      # shutdown_timeout: 5s

      ## Maximum number of requests permitted to be sent to the server concurrently.
      # concurrent_requests: 40

      ## Authorization token to be checked. If a token is set here the agents must
      ## send their token in the following format: Authorization: Bearer <secret-token>.
      ## It is recommended to use an authorization token in combination with SSL enabled.
      # secret_token:
      # ssl.enabled: false
      # ssl.certificate : "path/to/cert"
      # ssl.key : "path/to/private_key"

    queue: {}
      ## Queue type by name (default 'mem')
      ## The memory queue will present all available events (up to the outputs
      ## bulk_max_size) to the output, the moment the output is ready to server
      ## another batch of events.
      # mem:
      ## Max number of events the queue can buffer.
      # events: 4096

      ## Hints the minimum number of events stored in the queue,
      ## before providing a batch of events to the outputs.
      ## A value of 0 (the default) ensures events are immediately available
    ## to be sent to the outputs.
    # flush.min_events: 2048

    ## Maximum duration after which events are available to the outputs,
    ## if the number of events stored in the queue is < min_flush_events.
    # flush.timeout: 1s

    # When a key contains a period, use this format for setting values on the command line:
    # --set config."output\.file".enabled=false
    output.file:
      enabled: false
    #      path: "/usr/share/apm-server/data"
    #      filename: apm-server
    #      rotate_every_kb: 10000
    #      number_of_files: 5

    ## Set output.file.enabled to false to enable elasticsearch
    output.elasticsearch:
      hosts: ["elasticsearch-master:9200"]
      protocol: "http"
  #      protocol: "https"
  #      username: "elastic"
  #      password: "changeme"

  # List of beat plugins
  plugins: []
  # - kinesis.so

  # Additional container arguments
  extraArgs: []
  # - -d
  # - *

  # A map of additional environment variables
  extraVars: {}
  # test1: "test2"

  # Add additional volumes and mounts, for example to read other log files on the host
  extraVolumes: []
  # - hostPath:
  #     path: /var/log
  #   name: varlog
  extraVolumeMounts: []
  # - name: varlog
  #   mountPath: /host/var/log
  #   readOnly: true

  ## Labels to be added to pods
  podLabels: {}

  ## Annotations to be added to pods
  podAnnotations: {}

  resources: {}
    ## We usually recommend not to specify default resources and to leave this as a conscious
    ## choice for the user. This also increases chances charts run on environments with little
    ## resources, such as Minikube. If you do want to specify resources, uncomment the following
    ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
  #  cpu: 100m
  #  memory: 200Mi
  # requests:
  #  cpu: 100m
  #  memory: 100Mi

  ## Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  nodeSelector: {}

  # Tolerations for pod assignment
  # Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  tolerations: []

  ## Affinity configuration for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  affinity: {}

  rbac:
    # Specifies whether RBAC resources should be created
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:
